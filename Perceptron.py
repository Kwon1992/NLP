# https://mlwave.com/online-learning-perceptron/
# online learning perceptron
# 1943년도 고안된 artificial neuron...
# only using python internal function, modules...
# https://github.com/corazzon/KaggleStruggle/blob/master/word2vec-nlp-tutorial/MLWave-online-learning-perceptron.ipynb

"""
인공 신경망의 탄생 McCulloch-Pitts Neuron
 McCulloch는 신경 활동에 ‘all-or-nothing’활동이 있다고 추론했다. 활성화 임계 값에 도달하거나(출력 1) 출력하지 않으면 (출력 0) 신경세포가 발화한다.
 피츠 (Pitts)는 그러한 신경 학적 원칙을 사용하여 명제 논리를 포착 할 수 있는 잠재력을보고 이해했다.

예를 들어, 새가 씨앗과 같이 작은 물체를 먹으려고 한다. 이 행동을 표로 나타내면 다음과 같다.

OBJECT	    BROWN?	ROUND?	EAT?
Seed	    1	    1	    1
Leaf	    1	    0	    0
Golf Ball	0	    1	    0
Key	        0	    0	    0

* BROWN, ROUND는 feature가 될 것이다.

""AND 연산"" -  McCulloch-Pitts Neuron으로 위 표를 모델링 할 수 있다.
활성화 임계 값을 1.5로 설정하면 "BROWN"및 "ROUND"속성이 모두 충족 될 때만 뉴런이 실행된다.
그러면 입력 합계는 2 (1 + 1)로 1.5의 활성화 임계 값보다 크다.

Brown : 1, Round : 1 ==> [[2 > 1.5(임계값)??]] ==> OUTPUT : 1
"""


"""
Perceptron?
 가장 단순한 인공신경망이다.
 셉트론(perceptron)은 인공신경망의 한 종류로서, 1957년에 코넬 항공 연구소(Cornell Aeronautical Lab)의 프랑크 로젠블라트 (Frank Rosenblatt)에 의해 고안되었다. 
 이것은 가장 간단한 형태의 피드포워드(Feedforward) 네트워크 - 선형분류기- 으로도 볼 수 있다. 
 퍼셉트론 알고리즘은 하드웨어로 구현 된 최초의 인공 신경망이었다.
 
 퍼셉트론이 동작하는 방식은 
  각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 활성함수에 의해 판단되는데, 
  그 값이 임계치(보통 0)보다 크면 뉴런이 활성화되고 결과값으로 1을 출력한다. 뉴런이 활성화되지 않으면 결과값으로 -1을 출력한다.
  
   (마빈 민스키와 시모어 페퍼트는 저서 "퍼셉트론"에서 단층 퍼셉트론은 XOR 연산이 불가능하지만, 다층 퍼셉트론으로는 XOR 연산이 가능함을 보였다.)
   
   
 퍼셉트론은 지도학습 분류기의 일종이다. (이전 값에 대한 학습을 통해 예측을 하기 때문에)
 
 퍼셉트론은 들어오는 연결에 가중치를 할당하여 작동한다. 
 McCulloch-Pitts Neuron을 사용하여 들어오는 연결에서 값의 합계를 구하고 다음 특정 임계 값보다 높거나 낮은 지 확인했다. 
 내적을 구하는 대신 퍼셉트론을 사용했다. 여기에서는 들어오는 각 값에 가중치를 곱해서 합계를 구했다.

    sum: (value1 * weight1) + (value2 * weight2)

    weights[feature_index] += learning_rate * error * feature_value
"""


"""
인간의 신경 조직을 수학적으로 모델링하여 컴퓨터가 인간처럼 기억․학습․판단할 수 있도록 구현한 것이 인공 신경망 기술이다. 
신경 조직의 기본 단위는 뉴런인데, 인공 신경망에서는 뉴런의 기능을 수학적으로 모델링한 "퍼셉트론"을 기본 단위로 사용한다.

퍼셉트론은 입력값들을 받아들이는 여러 개의 입력 단자와 이 값을 처리하는 부분, 처리된 값을 내보내는 한 개의 출력 단자로 구성되어 있다. 
퍼셉트론은 각각의 입력 단자에 할당된 '가중치'를 입력값에 곱한 값들을 모두 합하여 가중합을 구한 후 
고정된 임계치보다 가중합이 작으면 0, 그렇지 않으면 1과 같은 방식으로 출력값을 내보낸다.

이러한 퍼셉트론은 "출력값에 따라 두 가지로만 구분하여 입력값들을 판정할 수 있을 뿐"이다.
이에 비해 복잡한 판정을 할 수 있는 인공 신경망은 다수의 퍼셉트론을 여러 계층으로 배열하여 
한 계층에서 출력된 신호가 다음 계층에 있는 모든 퍼셉트론의 입력 단자에 입력값으로 입력되는 구조로 이루어진다.

이러한 인공 신경망에서 가장 처음에 입력값을 받아들이는 퍼셉트론들을 입력층, 가장 마지막에 있는 퍼셉트론들을 출력층이라고 한다.

어떤 사진 속 물체의 색깔과 형태로부터 그 물체가 사과인지 아닌지를 구별할 수 있도록 인공 신경망을 학습시키는 경우를 생각해 보자. 
먼저 학습을 위한 입력값들 즉 학습 데이터를 만들어야 한다. 
학습 데이터를 만들기 위해서는 사과 사진을 준비하고 사진에 나타난 특징인 색깔과 형태를 수치화 해야 한다. 
이 경우 색깔과 형태라는 두 범주(feature)를 수치화하여 하나의 학습 데이터로 묶은 다음, ‘정답’에 해당하는 값과 함께 학습 데이터를 인공 신경망에 제공한다. 
이때 같은 범주에 속하는 입력값은 동일한 입력 단자를 통해 들어가도록 해야 한다. 
그리고 사과 사진에 대한 학습 데이터를 만들 때에 정답인 ‘사과이다’에 해당하는 값을 ‘1’로 설정하였다면 출력값 ‘0’은 ‘사과가 아니다’를 의미하게 된다.

인공 신경망의 작동은 크게 "학습 단계"와 "판정 단계"로 나뉜다.
"학습 단계"는 학습 데이터를 입력층의 입력 단자에 넣어 주고 출력층의 출력값을 구한 후, 이 출력값과 정답에 해당하는 값의 차이가 줄어들도록 가중치를 갱신하는 과정이다. 
어떤 학습 데이터가 주어지면 이때의 출력값을 구하고 학습 데이터와 함께 제공된 정답에 해당하는 값에서 출력값을 뺀 값 즉 오차 값을 구한다. 
이 오차 값의 일부가 출력층의 출력 단자에서 입력층의 입력 단자 방향으로 되돌아가면서 각 계층의 퍼셉트론별로 출력 신호를 만드는 데 관여한 모든 가중치들에 더해지는 방식으로 가중치들이 갱신된다.
이러한 과정을 다양한 학습 데이터에 대하여 반복하면 출력값들이 각각의 정답 값에 수렴하게 되고 판정 성능이 좋아진다. 
오차 값이 0에 근접하게 되거나 가중치의 갱신이 더 이상 이루어지지 않게 되면 학습 단계를 마치고 판정 단계로 전환한다. 

이때 판정의 오류를 줄이기 위해서는 학습 단계에서 대상들의 변별적 특징이 잘 반영되어 있는 서로 다른 학습 데이터를 사용하는 것이 좋다.
"""

